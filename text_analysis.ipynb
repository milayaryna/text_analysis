{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7b608e-385e-40de-99af-ba11c70d0852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU databricks-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a134c0-c975-47ef-a4bb-d354e7b572d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from pyspark.sql.functions import current_timestamp, year, month, add_months\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Workspace/Users/az.admz.yhting@hutchisonports.onmicrosoft.com\")\n",
    "from src.log import configure_logging\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91da408-68c8-4a64-b4a5-a1b39d7bb9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_previous_month():\n",
    "    df = spark.sql(\"SELECT add_months(current_timestamp(), -1) AS prev_month_time\")\n",
    "\n",
    "    result = df.select(\n",
    "        year(\"prev_month_time\").alias(\"year\"),\n",
    "        month(\"prev_month_time\").alias(\"month\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    return result[\"year\"], result[\"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696324c4-0d11-41f0-ace5-e6efa83df59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RawDataExtraction:\n",
    "    def __init__(self, year: int, month: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the RawDataExtraction class with year and month.\n",
    "        \n",
    "        Args:\n",
    "            year (int): The year for which data is to be extracted.\n",
    "            month (int): The month for which data is to be extracted.\n",
    "        \"\"\"\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "\n",
    "    def form_sql(self, table_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a SQL query string based on the provided table name and class attributes.\n",
    "        \"\"\"\n",
    "        sql = f\"\"\"\n",
    "        SELECT * FROM default.{table_name} \n",
    "        where YEAR = {self.year}  \n",
    "        AND MONTH = {self.month}\n",
    "        AND REMARKS IS NOT NULL\n",
    "        ORDER BY TML\n",
    "        \"\"\"\n",
    "        return sql\n",
    "    def get_cost_consumption(self) -> 'pd.DataFrame':\n",
    "        \"\"\"\n",
    "        Retrieve cost consumption data from the COS_RMPF_CC table using Spark SQL.\n",
    "        \"\"\"\n",
    "        df = spark.sql(self.form_sql(\"COS_RMPF_CC\"))\n",
    "        df = df.toPandas()\n",
    "        df_cost_consumption = df.drop(columns=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"], errors=\"ignore\")\n",
    "        return df_cost_consumption\n",
    "\n",
    "    def get_fleet_size(self) -> 'pd.DataFrame':\n",
    "        \"\"\"\n",
    "        Retrieve fleet size data from the COS_RMPF_CC table using Spark SQL.\n",
    "        \"\"\"\n",
    "        df = spark.sql(self.form_sql(\"COS_RMPF_FLEET_SIZE\"))\n",
    "        df = df.toPandas()\n",
    "        df_fleet_size = df.drop(columns=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"], errors=\"ignore\")\n",
    "        return df_fleet_size\n",
    "\n",
    "    def get_equipment_performance(self) -> 'pd.DataFrame':\n",
    "        \"\"\"\n",
    "        Retrieve equipment performance data from the COS_RMPF_CC table using Spark SQL.\n",
    "        \"\"\"\n",
    "        df = spark.sql(self.form_sql(\"COS_RMPF_EQUIPMENT_PERF\"))\n",
    "        df = df.toPandas()\n",
    "        df_equipment_performance = df.drop(columns=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"], errors=\"ignore\")\n",
    "        return df_equipment_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbd38d7-6f5d-4661-a094-b56cc18eca79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_others_cost_ratio(df: 'pd.DataFrame') -> 'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Calculate the ratio of 'Others' equipment type cost/consumption and update remarks columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame containing cost and consumption data.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Processed DataFrame with updated remarks for 'Others' exceeding 10% ratio.\n",
    "    \"\"\"\n",
    "    # Strip column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Group by TML and CATEGORY\n",
    "    grouped = df.groupby([\"TML\", \"CATEGORY\"])\n",
    "    final = pd.DataFrame()\n",
    "    \n",
    "    # Calculate total and 'Others' ratio for COST and CONSUMPTION\n",
    "    for i in [\"COST\", \"CONSUMPTION\"]:\n",
    "        total_cost = grouped[i].sum().rename(f\"total_{i}\")\n",
    "        others_cost = df[df[\"EQUIPMENT_TYPE\"] == \"Others\"].groupby([\"TML\", \"CATEGORY\"])[i].sum().rename(f\"others_{i}\")\n",
    "        result = pd.concat([total_cost, others_cost], axis=1).fillna(0)\n",
    "        # Avoid division by zero by setting ratio to 0 when total is 0\n",
    "        result[\"others_ratio\"] = result.apply(\n",
    "            lambda row: row[f\"others_{i}\"] / row[f\"total_{i}\"] if row[f\"total_{i}\"] != 0 else 0, axis=1\n",
    "        )\n",
    "        result = result[(result[\"others_ratio\"] > 0.1) & (result[\"others_ratio\"] <= 1)][[\"others_ratio\"]]\n",
    "        final = pd.concat([final, result], axis=0)\n",
    "    \n",
    "    # Reset index and add EQUIPMENT_TYPE\n",
    "    final.reset_index(inplace=True)\n",
    "    final.drop_duplicates([\"TML\", \"CATEGORY\"], keep=\"first\", inplace=True)\n",
    "    final[\"EQUIPMENT_TYPE\"] = \"Others\"\n",
    "    \n",
    "    # Merge with original DataFrame\n",
    "    df = df.merge(final, how=\"left\", on=[\"TML\", \"CATEGORY\", \"EQUIPMENT_TYPE\"])\n",
    "    \n",
    "    # Update REMARKS columns where others_ratio is not null\n",
    "    df.loc[pd.notnull(df[\"others_ratio\"]), \"REMARKS_CATEGORY\"] = \"additional_info\"\n",
    "    df.loc[pd.notnull(df[\"others_ratio\"]), \"REMARKS_SUBCATEGORY\"] = \"more_than_10_percent_cost\"\n",
    "    \n",
    "    # Drop temporary column\n",
    "    df = df.drop(columns=[\"others_ratio\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd7e659d-00d8-4688-af84-b094f71e8828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_data(df: 'pd.DataFrame', columns_to_keep: list, category_df: 'pd.DataFrame') -> tuple['pd.DataFrame', 'pd.DataFrame']:\n",
    "    \"\"\"\n",
    "    Process the given DataFrame to clean and classify remarks.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame to process.\n",
    "    - columns_to_keep (list): The list of columns to retain.\n",
    "    - category_df (pd.DataFrame): DataFrame containing remark categories.\n",
    "    \n",
    "    Returns:\n",
    "    - classified_df (pd.DataFrame): DataFrame with already classified remarks.\n",
    "    - unclassified_df (pd.DataFrame): DataFrame with unclassified remarks.\n",
    "    \"\"\"\n",
    "    # Preprocess category_df\n",
    "    filtered_clean_remarks_cats_df = category_df[['CLEAN_REMARKS', 'REMARKS_CATEGORY', 'REMARKS_SUBCATEGORY']].drop_duplicates(subset='CLEAN_REMARKS')\n",
    "    filtered_stemming_remarks_cats_df = category_df[['REMARK_STEMMING', 'REMARKS_CATEGORY', 'REMARKS_SUBCATEGORY']].dropna(subset=['REMARK_STEMMING'])\n",
    "    filtered_stemming_remarks_cats_df = filtered_stemming_remarks_cats_df.drop_duplicates(subset='REMARK_STEMMING')\n",
    "\n",
    "    # Clean REMARKS column: Remove numbers, spaces, commas, and dots, then standardize to lowercase\n",
    "    df[\"CLEAN_REMARKS\"] = df[\"REMARKS\"].astype(str).str.replace(r\"[^a-zA-Z]\", \"\", regex=True).str.strip().str.lower()\n",
    "    \n",
    "    # REMARKS stemming column\n",
    "    def stem_and_clean(text):\n",
    "        words = word_tokenize(str(text))\n",
    "        stemmed_words = [stemmer.stem(word) for word in words if word.isalpha()]\n",
    "        return ''.join(stemmed_words).lower()\n",
    "\n",
    "    df[\"REMARK_STEMMING\"] = df[\"REMARKS\"].apply(stem_and_clean)\n",
    "\n",
    "    # Remove duplicate CLEAN_REMARKS, keeping only the first occurrence\n",
    "    df_clean = df.loc[df['CLEAN_REMARKS'].drop_duplicates(keep='first').index]\n",
    "\n",
    "    # Retain specified columns plus CLEAN_REMARKS\n",
    "    df_clean = df_clean[columns_to_keep + [\"CLEAN_REMARKS\", \"REMARK_STEMMING\"]]\n",
    "    \n",
    "    # Stage 1: Merge with category_df to classify remarks based on CLEAN_REMARKS\n",
    "    merged_df = df_clean.merge(filtered_clean_remarks_cats_df, on=\"CLEAN_REMARKS\", how=\"left\")\n",
    "    \n",
    "    # Extract matched and unmatched\n",
    "    classified_df = merged_df.dropna(subset=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"])\n",
    "    unmatched_df = merged_df[merged_df[\"REMARKS_CATEGORY\"].isna()].drop(columns=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"])\n",
    "\n",
    "    # Stage 2: Match unmatched using REMARK_STEMMING\n",
    "    stemming_matched = unmatched_df.merge(filtered_stemming_remarks_cats_df, on=\"REMARK_STEMMING\", how=\"left\")\n",
    "\n",
    "    # Combine new matches with previously matched\n",
    "    newly_classified = stemming_matched.dropna(subset=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"])\n",
    "    still_unclassified = stemming_matched[stemming_matched[\"REMARKS_CATEGORY\"].isna()].drop(columns=[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"])\n",
    "\n",
    "    # Final combined classified and unclassified\n",
    "    final_classified = pd.concat([classified_df, newly_classified], ignore_index=True)\n",
    "    final_unclassified = still_unclassified\n",
    "    \n",
    "    return df, final_classified, final_unclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db165a54-77b1-4248-9ce3-9d1fd6025871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    we are a port container resources management company.\n",
    "    we have remarks columns and we want to classify the free text remarks into a category and subcategory.\n",
    "    If not certain, put under others. Each remark should have only one unique category and one unique subcategory. Do not assign more than one.\n",
    "\n",
    "    remark category and subcategory can only be one of the listed\n",
    "    | Category        | Subcategory                             | Description                                                                               |\n",
    "    |-----------------|--------------------------------------   |-------------------------------------------------------------------------------------------|\n",
    "    | additional_info | break_down_info                         | If there are mentions of failures or damages, they should be categorized under this type. |\n",
    "    |                 | cost_breakdown                          | If there are only mentions of monetary allocations or distributions of costs, they should be categorized under this type. |\n",
    "    |                 | potential_change_of_number_of_equipment | Indicates that the number of equipment may increase or decrease.                          |\n",
    "    |                 | usage                                   | Regarding the usage amount of the equipment, which may be measured in kilometers (km) or running hours (hr).    |\n",
    "    |                 | other                                   | Other relevant information.                                                               |\n",
    "    | justification   | financial_treatment                     | If there are mentions of how financial matters are handled, such as accruals, invoice reversals, release provisions, or accounting adjustments, they should be categorized under this type.|                                                 |\n",
    "    |                 | negative_figures                        | Explanation of negative values.                                                           |\n",
    "    |                 | high_low_cost_reason                    | Explanation for high or low costs.                                                        |\n",
    "    |                 | policy_56                               | If the remarks mention anything about Policy 56 to explain zero consumption or cost, it is because the amount was already charged at the time of purchase.|\n",
    "    |                 | special_cases                           | Unique or exceptional situations. Cases where consumption is zero.                        |\n",
    "    |                 | no_breakdown_or_activity_or_cost        | No equipment activity, no breakdown occurred, or no cost.                                 |\n",
    "    |                 | other                                   | Other relevant justifications.                                                            |\n",
    "    | other           | other                                   | Any other or uncertain how to calssified remarks.                                         |\n",
    "\n",
    "    ### Output format:\n",
    "    {{\n",
    "        \"category\": \"additional_info / justification / other\",\n",
    "        \"subcategory\": \"break_down_info / cost_breakdown / potential_change_of_number_of_equipment / usage / financial_treatment / negative_figures / high_low_cost_reason / policy_56 / special_cases / no_breakdown_or_activity / other\"\n",
    "    }}\n",
    "\n",
    "        \n",
    "    ### Here are some examples. Please follow the output format.\n",
    "\n",
    "    input1: \n",
    "    {{'CATEGORY': 'Cost_RMTotalCostByEquipmentType_ConsumablePurchase', 'EQUIPMENT_TYPE': 'AGV', 'COST': -107332.81, 'CONSUMPTION': nan, 'REMARKS': 'Equipment damage', 'CLEAN_REMARKS': 'equipmentdamage', 'REMARK_STEMMING': 'equipdam'}}\n",
    "    output1:\n",
    "    {{\n",
    "        \"category\": \"additional_info\",\n",
    "        \"subcategory\": \"break_down_info\"\n",
    "    }}\n",
    "\n",
    "    input2:\n",
    "    {{'CATEGORY': 'Consumption_WireRope', 'EQUIPMENT_TYPE': 'ASC', 'COST': 0.0, 'CONSUMPTION': 979.2, 'REMARKS': 'Policy 56/2021 - consumables items', 'CLEAN_REMARKS': 'policy/-consumablesitems', 'REMARK_STEMMING': 'policyconsumitem'}}\n",
    "    output2:\n",
    "    {{\n",
    "        \"category\": \"justification\",\n",
    "        \"subcategory\": \"policy_56\"\n",
    "    }}\n",
    "\n",
    "    input3:\n",
    "    {{'CATEGORY': 'Cost_RMTotalNonRecurrentCostByEquipmentType', 'EQUIPMENT_TYPE': 'QC', 'COST': -159166.3391, 'CONSUMPTION': nan, 'REMARKS': 'negative because of charching ECT for NRMC work during the year for the the cranes leased', 'CLEAN_REMARKS': 'negativebecauseofcharchingectfornrmcworkduringtheyearforthethecranesleased', 'REMARK_STEMMING': 'negbecausofcharchectfornrmcworkdurtheyearforthethecranleas'}}\n",
    "    output3:\n",
    "        {{\n",
    "        \"category\": \"justification\",\n",
    "        \"subcategory\": \"negative_figures\"\n",
    "    }}\n",
    "    \n",
    "    input4:\n",
    "    {{'CATEGORY': 'Cost_RMTotalNonRecurrentCostByEquipmentType', 'EQUIPMENT_TYPE': 'AGV', 'COST': 262303.0394, 'CONSUMPTION': nan, 'REMARKS': 'higher than R&M Total because of Capitalisation Third Party Services NRMC', 'CLEAN_REMARKS': 'higherthanr&mtotalbecauseofcapitalisationthirdpartyservicesnrmc', 'REMARK_STEMMING': 'highthanrmtotbecausofcapitthirdpartyservnrmc'}}\n",
    "    output4:\n",
    "        {{\n",
    "        \"category\": \"justification\",\n",
    "        \"subcategory\": \"high_low_cost_reason\"\n",
    "    }}\n",
    "\n",
    "    input5:\n",
    "    {{'CATEGORY': 'Cost_RMTotalCostByEquipmentType', 'EQUIPMENT_TYPE': 'Others', 'COST': 226426.06, 'CONSUMPTION': nan, 'REMARKS': '105.8K allocated to ENG general account | 120.6K allocated to site services general account', 'CLEAN_REMARKS': 'kallocatedtoenggeneralaccount|kallocatedtositeservicesgeneralaccount', 'REMARK_STEMMING': 'alloctoenggenaccountalloctositservgenaccount'}}\n",
    "    output5:\n",
    "        {{\n",
    "        \"category\": \"additional_info\",\n",
    "        \"subcategory\": \"cost_breakdown\"\n",
    "    }}\n",
    "\n",
    "    input6:\n",
    "    {{'CATEGORY': 'Cost_RMTotalCostByEquipmentType_ConsumablePurchase', 'EQUIPMENT_TYPE': 'Quay Deck & Yard', 'COST': 10, 'CONSUMPTION': nan, 'REMARKS': 'release of unrealised provisions for PO previous month', 'CLEAN_REMARKS': 'releaseofunrealisedprovisionsforpopreviousmonth', 'REMARK_STEMMING': 'releasofunrprovidforpoprevymon'}}\n",
    "    output6:\n",
    "        {{\n",
    "        \"category\": \"justification\",\n",
    "        \"subcategory\": \"financial_treatment\"\n",
    "    }}\n",
    "\n",
    "    input7:\n",
    "    {{'EQUIPMENT_TYPE': 'ASC', 'NUM_OF_EQUIPMENT': 142.0, 'MEASURING_UNIT': 'Equipment Moves', 'REMARKS': '10 ASCs out of ops for cost savings', 'CLEAN_REMARKS': 'ASCsoutofopsforcostsavings', 'REMARK_STEMMING': 'ascoutofopforcostsav'}}\n",
    "    output7:\n",
    "    {{\n",
    "        \"category\": \"additional_info\",\n",
    "        \"subcategory\": \"potential_change_of_number_of_equipment\"\n",
    "    }}\n",
    "    \n",
    "    input8:\n",
    "    {{'EQUIPMENT_TYPE': 'Empty Container Handler', 'NUM_OF_EQUIPMENT': 1.0, 'MEASURING_UNIT': 'Equipment Moves', 'REMARKS': 'Not utilized', 'CLEAN_REMARKS': 'Notutilized', 'REMARK_STEMMING': 'notutil'}}\n",
    "    output8:\n",
    "        {{\n",
    "        \"category\": \"justification\",\n",
    "        \"subcategory\": \"no_breakdown_or_activity_or_cost\"\n",
    "    }}\n",
    "\n",
    "    input9:\n",
    "    {{'EQUIPMENT_TYPE': 'AGV', 'NUM_OF_EQUIPMENT': 181.0, 'MEASURING_UNIT': 'Engine Running Hours', 'REMARKS': '96.022km in January', 'CLEAN_REMARKS': 'kminjanuary', 'REMARK_STEMMING': 'injanu'}}\n",
    "    output9:\n",
    "        {{\n",
    "        \"category\": \"additional_info\",\n",
    "        \"subcategory\": \"usage\"\n",
    "    }}\n",
    "\n",
    "    input10:\n",
    "    {{'EQUIPMENT_TYPE': 'Conventional RTGC', 'OPERATIONS_AVAILABILITY': 0.78, 'UTILIZATION_PCT': 0.57, 'MMBF': 545.3, 'MTTR': 36.6, 'REMARKS': '57.06%\\t\\tCH=\\t50.213%\\t   OS=\\t6.847%', 'CLEAN_REMARKS': 'chos', 'REMARK_STEMMING': ''}}\n",
    "    output10:\n",
    "        {{\n",
    "        \"category\": \"other\",\n",
    "        \"subcategory\": \"other\"\n",
    "    }}\n",
    "\n",
    "    input11:\n",
    "    {{'CATEGORY': 'Cost_RMTotalCostByEquipmentType', 'EQUIPMENT_TYPE': 'Others', 'COST': 752.4135, 'CONSUMPTION': nan, 'REMARKS': 'AP invoices & clearance cost', 'CLEAN_REMARKS': 'apinvoicesclearancecost', 'REMARK_STEMMING': 'apinvoclearcost'}}\n",
    "    output11:\n",
    "        {{\n",
    "        \"category\": \"other\",\n",
    "        \"subcategory\": \"other\"\n",
    "    }}\n",
    "\n",
    "    input12:\n",
    "    {{'CATEGORY': 'Consumption_Electricity_SelfGenRenewable', 'EQUIPMENT_TYPE': 'Others', 'COST': 0, 'CONSUMPTION': nan, 'REMARKS': 'No Partial meters available', 'CLEAN_REMARKS': 'nopartialmetersavailable', 'REMARK_STEMMING': 'nopartmetavail'}}\n",
    "    output12:\n",
    "        {{\n",
    "        \"category\": \"justification\",\n",
    "        \"subcategory\": \"no_breakdown_or_activity_or_cost\"\n",
    "    }}\n",
    "\n",
    "    input13:\n",
    "    {{'CATEGORY': 'Cost_RMTotalCostByEquipmentType', 'EQUIPMENT_TYPE': 'QC', 'COST': 40191.3075, 'CONSUMPTION': nan, 'REMARKS': 'Freight charges', 'CLEAN_REMARKS': 'freightcharges', 'REMARK_STEMMING': 'freightcharg'}}\n",
    "    output13:\n",
    "        {{\n",
    "        \"category\": \"other\",\n",
    "        \"subcategory\": \"other\"\n",
    "    }}\n",
    "\n",
    "    --- Additional reference rules ---\n",
    "    You may also use the following common keywords as a guide to help classification, but always consider the full semantic meaning of the remark:\n",
    "\n",
    "    - If remark includes:\n",
    "        - \"None fitted in period\"\n",
    "        - \"Equipment was not in operation\"\n",
    "        ➝ category: justification, subcategory: no_breakdown_or_activity_or_cost\n",
    "\n",
    "    - If remark includes:\n",
    "        - \"Claim refund\"\n",
    "        ➝ category: justification, subcategory: financial_treatment\n",
    "\n",
    "    - If remark includes:\n",
    "        - \"new in service\"\n",
    "        ➝ category: additional_info, subcategory: potential_change_of_number_of_equipment\n",
    "\n",
    "    - If remark includes:\n",
    "        - \"Difference due to\"\n",
    "        ➝ category: justification, subcategory: high_low_cost_reason\n",
    "\n",
    "    - If remark includes:\n",
    "        - \"TLC counter\", \"Hour counter\"\n",
    "        ➝ category: additional_info, subcategory: usage\n",
    "\n",
    "    Please remember, these are **reference only** and final classification must be based on meaning.\n",
    "\n",
    "\n",
    "    new input: \n",
    "    {data.to_dict()}  \n",
    "\n",
    "    your output:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6589ba85-cec1-4f17-8eb8-ed4931a8c98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def classify_remark(model: 'Any', row: 'pd.Series', existing_categories: list[str]) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Classify a remark using a provided model by generating a prompt and parsing the response.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Any): The model used to classify the remark (e.g., an AI model with an invoke method).\n",
    "        row (pd.Series): A row from a DataFrame containing the remark to classify.\n",
    "        existing_categories (list[str]): A list of valid categories for validation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[str, str]: A tuple containing:\n",
    "            - category (str): The classified category of the remark.\n",
    "            - subcategory (str): The classified subcategory of the remark.\n",
    "    \"\"\"\n",
    "    prompt =  generate_prompt(row)\n",
    "\n",
    "    # Initialize default values for category and subcategory\n",
    "    category, subcategory = \"other\", \"other\"\n",
    "\n",
    "    # Attempt classification up to 3 times to handle potential errors\n",
    "    for i in range(3): \n",
    "        try:\n",
    "            json_string = model.invoke(prompt).content\n",
    "\n",
    "            # Clean the JSON string\n",
    "            cleaned_string = json_string.replace('```json', '').replace('```', '').replace('\\\\n', '\\\\\\\\n').strip()\n",
    "            cleaned_string = cleaned_string.rstrip(',')\n",
    "            match = re.search(r'\\{.*\\}', cleaned_string, re.DOTALL)\n",
    "            if match:\n",
    "                cleaned_string = match.group(0)\n",
    "\n",
    "            # Parse the cleaned JSON string into a dictionary\n",
    "            result = json.loads(cleaned_string)\n",
    "            category = result.get(\"category\", \"other\")\n",
    "            subcategory = result.get(\"subcategory\", \"other\")\n",
    "\n",
    "            # Validate if the category and subcategory are in the existing categories\n",
    "            if category in existing_categories and subcategory in existing_categories[category]:\n",
    "                return category, subcategory  \n",
    "\n",
    "            # If invalid, log a message and retry\n",
    "            logger.info(f\"Invalid category/subcategory: {category}/{subcategory}, retrying...\")\n",
    "            print(f\"Invalid category/subcategory: {category}/{subcategory}, retrying...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            logger.error(f\"Error encountered: {str(e)}\")\n",
    "            print(f\"Error encountered: {str(e)}\")\n",
    "\n",
    "            # If the error is related to safety filters, return defaults immediately\n",
    "            if \"safety filters\" in str(e).lower():\n",
    "                return \"other\", \"other\"\n",
    "            \n",
    "    # After 3 failed attempts, return the default values for category and subcategory\n",
    "    return category, subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a306e7aa-f662-47a4-8815-248312335e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_table(table: 'pd.DataFrame') -> 'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Process a DataFrame table by classifying remarks and adding category columns.\n",
    "    \"\"\"\n",
    "    # Check if the table is empty; if so, add empty category columns and return\n",
    "    if table.empty:\n",
    "        table[\"REMARKS_CATEGORY\"] = pd.Series(dtype=\"object\")\n",
    "        table[\"REMARKS_SUBCATEGORY\"] = pd.Series(dtype=\"object\")\n",
    "        return table\n",
    "    \n",
    "    # Apply the classify_remark function to each row and assign results to new columns\n",
    "    table[[\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"]] = table.apply(\n",
    "        lambda row: pd.Series(classify_remark(LLM_MODEL, row, existing_categories)), axis=1\n",
    "    )\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f02cfa-7d5d-425c-a549-3408ee37a59e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_save_file(file_type: str, df_original: 'pd.DataFrame', classified_remarks: 'pd.DataFrame', processed_unclassified_remarks: 'pd.DataFrame', year: int, month: int, bu: str, classified_by_ratio: 'pd.DataFrame' = None) -> None:\n",
    "    \"\"\"\n",
    "    Processes and saves categorized remark data for a specific file type.\n",
    "\n",
    "    Parameters:\n",
    "    - file_type (str): The type of data being processed (e.g., 'cost_consumption', 'fleet_size').\n",
    "    - df_original (pd.DataFrame): The original DataFrame before classification.\n",
    "    - classified_remarks (pd.DataFrame): DataFrame containing already classified remarks.\n",
    "    - processed_unclassified_remarks (pd.DataFrame): DataFrame containing processed unclassified remarks.\n",
    "    - year (int): Year of the dataset.\n",
    "    - month (int): Month of the dataset.\n",
    "    - bu (str): Business unit identifier.\n",
    "    - classified_by_ratio (pd.DataFrame, optional): DataFrame containing classified remarks by ratio (only for cost_consumption).\n",
    "    \"\"\"    \n",
    "    # Merge classified_remarks and processed_unclassified_remarks\n",
    "    merged_df = pd.concat([classified_remarks, processed_unclassified_remarks], ignore_index=True)\n",
    "\n",
    "    # Perform a left join with the original DataFrame to include category columns\n",
    "    categorized_df = df_original.merge(\n",
    "        merged_df[['CLEAN_REMARKS', 'REMARKS_CATEGORY', 'REMARKS_SUBCATEGORY']], \n",
    "        on='CLEAN_REMARKS', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop \"CLEAN_REMARKS\" and \"REMARKS_STEMMING\" column\n",
    "    categorized_df = categorized_df.drop(columns=['CLEAN_REMARKS', 'REMARK_STEMMING'])\n",
    "\n",
    "    # Fill NaN with \"other\"\n",
    "    categorized_df[['REMARKS_CATEGORY', 'REMARKS_SUBCATEGORY']] = categorized_df[['REMARKS_CATEGORY', 'REMARKS_SUBCATEGORY']].fillna(\"other\")\n",
    "\n",
    "    # Add ratio columns to categorized_df (only for cost_consumption)\n",
    "    if file_type == \"cost_consumption\" and classified_by_ratio is not None:\n",
    "        categorized_df = pd.concat([categorized_df, classified_by_ratio], ignore_index=True)\n",
    "\n",
    "    # Convert Pandas DataFrame to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(categorized_df)\n",
    "\n",
    "    # Register the Spark DataFrame as a temporary view\n",
    "    spark_df.createOrReplaceTempView(\"temp_updates\")\n",
    "\n",
    "    # Define the target table based on file_type\n",
    "    table_mapping = {\n",
    "        \"cost_consumption\": {\n",
    "            \"table\": \"default.COS_RMPF_CC\",\n",
    "            \"merge_condition\": \"\"\"\n",
    "                target.TML = source.TML AND\n",
    "                target.YEAR = source.YEAR AND\n",
    "                target.MONTH = source.MONTH AND\n",
    "                target.CATEGORY = source.CATEGORY AND\n",
    "                target.EQUIPMENT_TYPE = source.EQUIPMENT_TYPE AND\n",
    "                target.REMARKS = source.REMARKS AND\n",
    "                target.CREATE_DATE = source.CREATE_DATE\n",
    "            \"\"\"\n",
    "        },\n",
    "        \"fleet_size\": {\n",
    "            \"table\": \"default.COS_RMPF_FLEET_SIZE\",\n",
    "            \"merge_condition\": \"\"\"\n",
    "                target.TML = source.TML AND\n",
    "                target.YEAR = source.YEAR AND\n",
    "                target.MONTH = source.MONTH AND\n",
    "                target.EQUIPMENT_TYPE = source.EQUIPMENT_TYPE AND\n",
    "                target.REMARKS = source.REMARKS AND\n",
    "                target.CREATE_DATE = source.CREATE_DATE\n",
    "            \"\"\"\n",
    "        },\n",
    "        \"equipment_performance\": {\n",
    "            \"table\": \"default.COS_RMPF_EQUIPMENT_PERF\",\n",
    "            \"merge_condition\": \"\"\"\n",
    "                target.TML = source.TML AND\n",
    "                target.YEAR = source.YEAR AND\n",
    "                target.MONTH = source.MONTH AND\n",
    "                target.EQUIPMENT_TYPE = source.EQUIPMENT_TYPE AND\n",
    "                target.REMARKS = source.REMARKS AND\n",
    "                target.CREATE_DATE = source.CREATE_DATE\n",
    "            \"\"\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    table_info = table_mapping.get(file_type)\n",
    "    target_table = table_info[\"table\"]\n",
    "    merge_condition = table_info[\"merge_condition\"]\n",
    "\n",
    "    # Use MERGE INTO to update the original table\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {target_table} AS target\n",
    "        USING temp_updates AS source\n",
    "        ON {merge_condition}\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET \n",
    "                target.REMARKS_CATEGORY = source.REMARKS_CATEGORY,\n",
    "                target.REMARKS_SUBCATEGORY = source.REMARKS_SUBCATEGORY\n",
    "    \"\"\")\n",
    "\n",
    "    # Print confirmation message upon successful update\n",
    "    logger.info(f\"{year}_{month}_{bu}_Categorized_{file_type}_data updated to {target_table}!\")\n",
    "\n",
    "    # Save to CSV\n",
    "    notebook_path = f\"file:/Workspace/Users/az.admz.yhting@hutchisonports.onmicrosoft.com/data/output/{year}_{month}_{bu}_Categorized_{file_type}_data.csv\"\n",
    "    dbutils.fs.put(notebook_path, spark_df.toPandas().to_csv(index=False, header=True), True)\n",
    "\n",
    "    # Print confirmation message upon successful save\n",
    "    logger.info(f\"{year}_{month}_{bu}_Categorized_{file_type}_data.csv done!\")\n",
    "    print(f\"✅ {year}_{month}_{bu}_Categorized_{file_type}_data.csv done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26be64ec-b118-4820-8475-6adf4a8a30be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/1307/command-7403283837499845-2576223102:13: DeprecationWarning: Currently, temperature defaults to 0.0 if not specified. In the next release, temperature will need to be explicitly set. Please update your code to specify a temperature value. Note: If you are using an o1 or o3 model, you need to set temperature=None.\n  LLM_MODEL = ChatDatabricks(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:149: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [YEAR, MONTH, COST, CONSUMPTION] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:149: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [YEAR, MONTH, NUM_OF_EQUIPMENT, TOTAL_USAGE, AVG_AGE, NUM_LEASE_EQUIP, NUM_EQUIP_IN_USE, NUM_LEASE_EQUIP_IN_USE, NUM_EQUIP_NOT_IN_USE, NUM_LEASE_EQUIP_NOT_IN_USE, USAGE_TOTAL_OWN_EQUIP, USAGE_TOTAL_LEASE_EQUIP, AVG_AGE_OWN_EQUIP_IN_USE, AVG_AGE_OWN_EQUIP_NOT_IN_USE, USAGE_TOTAL_OWN_EQUIP_OMU, USAGE_TOTAL_LEASE_EQUIP_OMU] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n/databricks/spark/python/pyspark/sql/pandas/utils.py:149: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [YEAR, MONTH, OPERATIONS_AVAILABILITY, UTILIZATION_PCT, MMBF, MTTR] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n2025-05-02 06:39:22,648 text_analysis INFO 2025_1_0502_Categorized_cost_consumption_data updated to default.COS_RMPF_CC!\n/databricks/spark/python/pyspark/sql/pandas/utils.py:149: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [YEAR, MONTH, COST, CONSUMPTION] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 567733 bytes.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 06:39:28,506 text_analysis INFO 2025_1_0502_Categorized_cost_consumption_data.csv done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2025_1_0502_Categorized_cost_consumption_data.csv done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 06:39:36,654 text_analysis INFO 2025_1_0502_Categorized_fleet_size_data updated to default.COS_RMPF_FLEET_SIZE!\n/databricks/spark/python/pyspark/sql/pandas/utils.py:149: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [YEAR, MONTH, NUM_OF_EQUIPMENT, NUM_LEASE_EQUIP, NUM_EQUIP_IN_USE, NUM_LEASE_EQUIP_IN_USE, NUM_EQUIP_NOT_IN_USE, NUM_LEASE_EQUIP_NOT_IN_USE, USAGE_TOTAL_OWN_EQUIP, USAGE_TOTAL_LEASE_EQUIP, AVG_AGE_OWN_EQUIP_IN_USE, AVG_AGE_OWN_EQUIP_NOT_IN_USE, USAGE_TOTAL_OWN_EQUIP_OMU, USAGE_TOTAL_LEASE_EQUIP_OMU] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 27297 bytes.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 06:39:39,033 text_analysis INFO 2025_1_0502_Categorized_fleet_size_data.csv done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2025_1_0502_Categorized_fleet_size_data.csv done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 06:39:46,334 text_analysis INFO 2025_1_0502_Categorized_equipment_performance_data updated to default.COS_RMPF_EQUIPMENT_PERF!\n/databricks/spark/python/pyspark/sql/pandas/utils.py:149: UserWarning: The conversion of DecimalType columns is inefficient and may take a long time. Column names: [YEAR, MONTH, OPERATIONS_AVAILABILITY, UTILIZATION_PCT, MMBF, MTTR] If those columns are not necessary, you may consider dropping them or converting to primitive types before the conversion.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 16466 bytes.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 06:39:48,702 text_analysis INFO 2025_1_0502_Categorized_equipment_performance_data.csv done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2025_1_0502_Categorized_equipment_performance_data.csv done!\n"
     ]
    }
   ],
   "source": [
    "BU = \"0502\"\n",
    "#YEAR, MONTH = get_previous_month()\n",
    "YEAR = 2025\n",
    "MONTH = 1\n",
    "\n",
    "# log\n",
    "logger = configure_logging(\"text_analysis\")\n",
    "\n",
    "# Lancaster Stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# LLM model\n",
    "LLM_MODEL = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-3-70b-instruct\"\n",
    ")\n",
    "# Load remark categories\n",
    "all_clean_remarks_categories_df = pd.read_csv(\"file:/Workspace/Users/az.admz.yhting@hutchisonports.onmicrosoft.com/data/raw/all_clean_remarks_categories_new.csv\")\n",
    "\n",
    "# columns to keep\n",
    "cost_consumption_keep = [\"CATEGORY\", \"EQUIPMENT_TYPE\", \"COST\", \"CONSUMPTION\", \"REMARKS\"]\n",
    "fleet_size_keep = [\"EQUIPMENT_TYPE\", \"NUM_OF_EQUIPMENT\", \"MEASURING_UNIT\", \"REMARKS\"]\n",
    "equipment_performance_keep = [\"EQUIPMENT_TYPE\", \"OPERATIONS_AVAILABILITY\", \"UTILIZATION_PCT\", \"MMBF\", \"MTTR\", \"REMARKS\"]\n",
    "\n",
    "# Pre-define remark categories\n",
    "existing_categories = {\n",
    "    \"additional_info\": [\"break_down_info\", \"cost_breakdown\", \"potential_change_of_number_of_equipment\", \"usage\", \"other\"],\n",
    "    \"justification\": [\"financial_treatment\", \"negative_figures\", \"high_low_cost_reason\", \"policy_56\", \"special_cases\", \"no_breakdown_or_activity_or_cost\", \"other\"],\n",
    "    \"other\": [\"other\"]\n",
    "}\n",
    "\n",
    "\n",
    "def executor(year: int, month: int, bu: str) -> None:\n",
    "    \n",
    "    # Get raw df\n",
    "    row_data_loader_object = RawDataExtraction(year=year, month=month)\n",
    "    df_cost_consumption = row_data_loader_object.get_cost_consumption()\n",
    "    df_fleet_size = row_data_loader_object.get_fleet_size()\n",
    "    df_equipment_performance = row_data_loader_object.get_equipment_performance()\n",
    "    \n",
    "    # Process cost consumption for 'Others' ratio > 10%\n",
    "    df_cost_consumption_processed = calculate_others_cost_ratio(df_cost_consumption)\n",
    "    \n",
    "    # Split processed cost consumption into classified and unclassified based on REMARKS_CATEGORY and REMARKS_SUBCATEGORY\n",
    "    df_cost_consumption_classified_by_ratio = df_cost_consumption_processed[\n",
    "        pd.notnull(df_cost_consumption_processed[\"REMARKS_CATEGORY\"]) & \n",
    "        pd.notnull(df_cost_consumption_processed[\"REMARKS_SUBCATEGORY\"])\n",
    "    ]\n",
    "    df_cost_consumption_unclassified_by_ratio = df_cost_consumption_processed[\n",
    "        pd.isnull(df_cost_consumption_processed[\"REMARKS_CATEGORY\"]) | \n",
    "        pd.isnull(df_cost_consumption_processed[\"REMARKS_SUBCATEGORY\"])\n",
    "    ]\n",
    "\n",
    "    # Remove category and subcategory columns from df_cost_consumption_classified_by_ratio\n",
    "    df_cost_consumption_unclassified_by_ratio = df_cost_consumption_unclassified_by_ratio.drop(\n",
    "    [\"REMARKS_CATEGORY\", \"REMARKS_SUBCATEGORY\"], axis=1)\n",
    "    \n",
    "    # Get classified df and unclassified df from the remaining data\n",
    "    df_cost_consumption, df_classified_cost_consumption_clean, df_unclassified_cost_consumption_clean = process_data(\n",
    "        df_cost_consumption_unclassified_by_ratio, cost_consumption_keep, all_clean_remarks_categories_df\n",
    "    )\n",
    "    df_fleet_size, df_classified_fleet_size_clean, df_unclassified_fleet_size_clean = process_data(\n",
    "        df_fleet_size, fleet_size_keep, all_clean_remarks_categories_df\n",
    "    )\n",
    "    df_equipment_performance, df_classified_equipment_performance_clean, df_unclassified_equipment_performance_clean = process_data(\n",
    "        df_equipment_performance, equipment_performance_keep, all_clean_remarks_categories_df\n",
    "    )\n",
    "\n",
    "    # Classify remarks with LLM\n",
    "    df_processed_unclassified_cost_consumption_clean = process_table(df_unclassified_cost_consumption_clean)\n",
    "    df_processed_unclassified_fleet_size_clean = process_table(df_unclassified_fleet_size_clean)\n",
    "    df_processed_unclassified_equipment_performance_clean = process_table(df_unclassified_equipment_performance_clean)\n",
    "   \n",
    "    # Process and save data\n",
    "    process_and_save_file(\"cost_consumption\", df_cost_consumption, df_classified_cost_consumption_clean, df_processed_unclassified_cost_consumption_clean, year, month, bu, df_cost_consumption_classified_by_ratio)\n",
    "    process_and_save_file(\"fleet_size\", df_fleet_size, df_classified_fleet_size_clean, df_processed_unclassified_fleet_size_clean, year, month, bu)\n",
    "    process_and_save_file(\"equipment_performance\", df_equipment_performance, df_classified_equipment_performance_clean, df_processed_unclassified_equipment_performance_clean, year, month, bu)\n",
    "    \n",
    "executor(YEAR, MONTH, BU)\n",
    "\n",
    "def process_json_and_execute():\n",
    "    json_path = \"file:/Workspace/Users/az.admz.yhting@hutchisonports.onmicrosoft.com/data/output/updated_tml_record.json\"\n",
    "    lines = spark.read.text(json_path).collect()\n",
    "    json_str = \"\\n\".join([row.value for row in lines])\n",
    "    data = json.loads(json_str)\n",
    "    \n",
    "    if \"COS_RMPF\" in data:\n",
    "        executor(YEAR, MONTH, BU)\n",
    "\n",
    "#process_json_and_execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5648947207875535,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "text_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}